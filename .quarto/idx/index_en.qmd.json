{"title":"Monte Carlo simulation","markdown":{"yaml":{"title-block-banner":"#54698a","title-block-banner-color":"#dee1e3","title":"Monte Carlo simulation","subtitle":"Example","author":"Michał Kołodziejczyk","date":"now","format":{"html":{"toc":true,"toc-location":"left","tbl-pos":"H","code-fold":true,"code-summary":"Script:"}},"editor":"visual","css":"styles.css","jupyter":"python3","cap-location":"margin"},"headingText":"Functions","containsRefs":false,"markdown":"\n\nMonte Carlo simulation is a method used to predict the probability of an outcome when there is variability in the data. The variability in the data can be described in several ways. Most commonly, statistical methods are used, and in the case of a quantity with a normal distribution, its variability is described by the mean and standard deviation.\n\nIn a Monte Carlo experiment, a large number of data are drawn from an assumed range of variability of a particular model quantity. These data are then used to generate a large number of model responses, and the resulting distribution of results is evaluated using statistical methods.\n\nThe idea of the Monte Carlo method is outlined in the figure below for a very simple model described by an ordinary function of several independent variables $x_i$. For the sake of simplicity, the distribution of each of these variables is assumed to be Gaussian.\n\n![The idea of the Monte Carlo method](idea.png){#fig-11 fig-align=\"center\" width=\"496\"}\n\nThe data sampling scheme is as follows:\n\n-   Using a random number generator, a probability $p$ is drawn for each input data $x_i$,\n-   Then the current value of the data $x_i$ is determined using the inverse Gaussian cumulative distribution function.\n\nThe figure below illustrates this process.\n\n![The data sampling scheme of $x_i$](sampling.png){#fig-22 fig-align=\"center\" width=\"490\"}\n\n\n## The model\n\nThe model is described by a function of three independent variables $x_1, x_2, x_3$:\n\n$y = a + b \\: sin(x_1) + c\\: x_2^2 + d \\: ln(x_3)$,\n\nwhere $a,\\: b,\\: c$ i $d$ are constant parameters.\n\n```{python}\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, ks_2samp, shapiro\nimport math\n\n# model\ndef funkcja_model(param, x):\n    a = param[0]\n    b = param[1]\n    c = param[2]\n    d = param[3]\n    # model\n    y = a + b*math.sin(x[0]) + c * (x[1] ** 2) +d * math.log(x[2])\n    return y\n\n```\n\n## Charts\n\nThe 'wykresy' function plots the histogram, probability density distribution, and cumulative distribution of each variable. Normal distributions are assumed for simplicity.\n\n```{python}\n\ndef wykresy(i, x, sd):\n        plt.subplot(1, 3, 1)\n        plt.hist(np.random.normal(x[i], sd[i], 10000), bins=30, alpha=0.7, color=\"#54698a\")\n        plt.xlabel(f\"x{i+1}\")\n        plt.ylabel(\"frequency\")\n        plt.title(\"Histogram\")\n\n        xx = np.linspace(x[i] - 4 * sd[i], x[i] + 4 * sd[i], 100)\n        yy = norm.pdf(xx, x[i], sd[i])\n        plt.subplot(1, 3, 2)\n        plt.plot(xx, yy, lw=2, color=\"#54698a\")\n        plt.xlabel(f\"x{i+1}\")\n        plt.ylabel(\"probability density\")\n        plt.title(f\"Distribution of x{i+1}\")\n\n        yy = norm.cdf(xx, x[i], sd[i])\n        plt.subplot(1, 3, 3)\n        plt.plot(xx, yy, lw=2, color=\"#54698a\")\n        plt.xlabel(f\"x{i+1}\")\n        plt.ylabel(\"cumulative probability\")\n        plt.title(\"cdf\")\n        plt.show()\n\n```\n\n## Monte Carlo\n\nFunction that implements a Monte Carlo experiment. Formal parameters: n - number of variables $x_i$, nn - number of draws, x - mean values of the $x_i$, sd - their standard deviations, parametry - parameters of the function describing the model. The function uses a random number generator and the inverse function of the cumulative distribution function.\n\n```{python}\n\ndef monte_carlo(n, nn, x, sd, parametry):\n    np.random.seed(7)\n    y = np.zeros(nn)\n    for i in range(nn):\n        los_x = np.zeros(n)\n        for j in range(n):\n            los_x[j] = norm.ppf(np.random.rand(), x[j], sd[j])\n        y[i] = funkcja_model(parametry, los_x)\n    return y\n\n```\n\n# The data\n\nExample data.\n\n```{python}\nimport pandas as pd\nfrom itables import show    \n\nn = 3  # the number of independent variables\nparametry = [1, 2, 3, 4]\n\nx1 = 10\nsd1 = 0.51\n\nx2 = 15\nsd2 = 0.5\n\nx3 = 5\nsd3 = 0.1\n\nx = [x1, x2, x3]\nsd = [sd1, sd2, sd3]\n\ndane_x = pd.DataFrame(x)\ndane_sd = pd.DataFrame(sd)\n\ndane = pd.concat([dane_x, dane_sd], axis=1)\ndane.columns = ['x','std']\ndane.index = ['x1','x2','x3']\n\nshow(np.transpose(dane))\n\n```\n\n## Data distributions\n\nInput data distributions.\n\n```{python}\n#| label: fig-1\n#| fig-cap: \n#|   - \"Distribution of x1\"\n#|   - \"Distribution of x2\"\n#|   - \"Distribution of x3\"\n#| fig-cap-location: margin\n#| message: false\n#| warning: false\n\nfor i in range(n):\n    wykresy(i, x, sd)\n\n```\n\n# Monte Carlo Simulation\n\nNumber of draws assumed: $nn = 100 \\: 000$.\n\n## Monte Carlo simulation results\n\n```{python}\n\nnn = 100000  # the number od draws\n\ny = monte_carlo(n, nn, x, sd, parametry)  #  Monte Carlo simulation\n\n```\n\nAll the results of the model calculations are shown in @fig-2 . Their mean and standard deviation are given below.\n\n```{python}\n#| label: fig-2\n#| fig-cap: \"Results of sampling in Monte Carlo simulation \"\n#| fig-cap-location: margin\n#| message: false\n#| warning: false\n\nplt.scatter(range(len(y)), y, s=0.5, color=\"#54698a\")\nplt.xlabel(\"draw number\")\nplt.ylabel(\"y\")\nplt.title(\"Summary Result of Monte Carlo Simulation\")\nplt.show()\n\ny_av = np.mean(y)\nsd_y = np.std(y)\n\nprint(\"The resulting mean y_av = \", round(y_av,2))\nprint(\"Standard deviation sd_y = \", round(sd_y,2))\n\n```\n\n## Distribution of the resulting quantity\n\n@fig-3 shows a histogram and a $y$ quantile plot for visual evaluation of the distribution.\n\n### Histogram and quantile plot\n\nThe histogram allows you to assess the symmetry of the distribution, and the quantile plot allows you to assess whether the distribution is normal.\n\n```{python}\n#| label: fig-3\n#| fig-cap: \"Visual check of the normality of the y-distribution \"\n#| fig-cap-location: margin\n#| message: false\n#| warning: false\n\nplt.subplot(1, 2, 1)\nplt.hist(y, bins=30, color=\"#54698a\", alpha=0.7)\nplt.title(\"Histogram of y\")\nplt.ylabel(\"frequency\")\nplt.xlabel(\"y\")\n\nyy = (y - y_av) / sd_y  # Standardization of distribution\n\nplt.subplot(1, 2, 2)\nplt.scatter(np.sort(norm.ppf(np.linspace(0.01, 0.99, len(yy)))), np.sort(yy), s=0.5, color=\"#54698a\")\nplt.title('Quantile plot Q-Q')\nplt.xlabel('quantiles of the normal distribution')\nplt.ylabel('quantiles of the y-distribution')\nplt.plot([min(yy), max(yy)], [min(yy), max(yy)], color='#96cdf2')\nplt.show()\n\n```\n\n### Check the normality of the y-distribution\n\nThe $y$ has 100,000 elements, so the Kolmogorov-Smirnov test was used to assess the normality of its distribution..\n\n```{python}\n\n# Kolmogorow-Smirnow test\nksx = ks_2samp(y, np.random.normal(y_av, sd_y, len(y)))\nksxp = ksx.pvalue\nprint(\" p_value = \", round(ksx.pvalue,4))\n\nif ksxp < 0.05:\n    print(\"\\nAccording to the Kolmogorov-Smirnov test, the distribution of y-size is not normal,\\nbecause the probability of falsely negating its normality  \\np_value is < 0.05. \")\nelse:\n    print(\"According to the Kolmogorov-Smirnov test, the distribution of y is normal (the normality of the distribution cannot be denied)\\n\\n\")\n\n# Shapiro-Wilka test\nif nn <= 5000:\n    swx = shapiro(y)\n    print(swx)\n\n    swxp = swx.pvalue\n\n    if swxp < 0.05:\n        print(\"According to the Shapiro-Wilk test, the distribution of y-size is not normal\\n\\n\")\n    else:\n        print(\"According to the Shapiro-Wilk test, the distribution of y is normal (the normality of the distribution cannot be denied)\\n\\n\")\n\n```\n\n### The distribution of y\n\n@fig-4 represents the obtained probability density distribution $y$ and its cumulative distribution function.\n\n```{python}\n#| label: fig-4\n#| fig-cap: \"The distribution of y \"\n#| tbl-cap-location: margin\n#| message: false\n#| warning: false\n\nplt.subplot(1, 2, 1)\nd = np.histogram(y, bins=50, density=True)\nplt.title(\"The distribution of y\")\nplt.xlabel(\"y\")\nplt.ylabel(\"probability density\")\nplt.plot(d[1][:-1], d[0], color=\"#54698a\", lw=2)\n\nplt.subplot(1, 2, 2)\nplt.plot(d[1][:-1], np.cumsum(d[0]) / max(np.cumsum(d[0])), lw=2, color=\"#54698a\")\nplt.xlabel(\"y\")\nplt.ylabel(\"cumulative probability\")\nplt.title(\"Cumulative distribution function\")\nplt.show()\n\n```\n\n### Median, skewness and kurtosis\n\nThe median, skewness, and kurtosis of $y$ were determined to further assess the deviation from the normal distribution.\n\n```{python}\n\nfrom scipy.stats import skew\nfrom scipy.stats import kurtosis\n\n\nmed = np.median(y,  axis=0)\nprint(\"Median = \", round(med,2))\n\nsk = skew(y, axis=0, bias=True)\nprint(\"Skewness = \", round(sk,5))\n\nkurt = kurtosis(y, fisher=True, axis=0, bias=True) # kurtoza = 0 dla rozkłądu Gaussa\nprint(\"Kortosis = \", round(kurt,5))\n\nq_025 = np.quantile(y, 0.025)\nq_975 = np.quantile(y, 0.975)\n\nprint(\"0.025 quantile = \", round(q_025,2))\nprint(\"0.975 quantile = \", round(q_975,2))\n```\n\n# Conclusion\n\nAll inputs $x_1,\\: x_2$ and $x_3$ follow a Gaussian distribution. The distribution of the result $y$ deviates slightly from the normal distribution. The skewness is small. The median is close to the mean. The kurtosis is slightly positive, which means that the distribution is slightly narrower than normal. The deviation from the normal distribution occurs in the 'tails' of the distribution, as can be clearly seen in the Q-Q plot in @fig-3 . The results are slightly more assembled around the mean than with the Gaussian distribution.\n\nThe variability of the data $x_1,\\: x_2$ and $x_3$ means that the result of the model's performance will be in the range of 596.13 to 771.98 with 95% confidence, i.e. $y = 682.4_{-86.3}^{+89.6}$.\n","srcMarkdownNoYaml":"\n\nMonte Carlo simulation is a method used to predict the probability of an outcome when there is variability in the data. The variability in the data can be described in several ways. Most commonly, statistical methods are used, and in the case of a quantity with a normal distribution, its variability is described by the mean and standard deviation.\n\nIn a Monte Carlo experiment, a large number of data are drawn from an assumed range of variability of a particular model quantity. These data are then used to generate a large number of model responses, and the resulting distribution of results is evaluated using statistical methods.\n\nThe idea of the Monte Carlo method is outlined in the figure below for a very simple model described by an ordinary function of several independent variables $x_i$. For the sake of simplicity, the distribution of each of these variables is assumed to be Gaussian.\n\n![The idea of the Monte Carlo method](idea.png){#fig-11 fig-align=\"center\" width=\"496\"}\n\nThe data sampling scheme is as follows:\n\n-   Using a random number generator, a probability $p$ is drawn for each input data $x_i$,\n-   Then the current value of the data $x_i$ is determined using the inverse Gaussian cumulative distribution function.\n\nThe figure below illustrates this process.\n\n![The data sampling scheme of $x_i$](sampling.png){#fig-22 fig-align=\"center\" width=\"490\"}\n\n# Functions\n\n## The model\n\nThe model is described by a function of three independent variables $x_1, x_2, x_3$:\n\n$y = a + b \\: sin(x_1) + c\\: x_2^2 + d \\: ln(x_3)$,\n\nwhere $a,\\: b,\\: c$ i $d$ are constant parameters.\n\n```{python}\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, ks_2samp, shapiro\nimport math\n\n# model\ndef funkcja_model(param, x):\n    a = param[0]\n    b = param[1]\n    c = param[2]\n    d = param[3]\n    # model\n    y = a + b*math.sin(x[0]) + c * (x[1] ** 2) +d * math.log(x[2])\n    return y\n\n```\n\n## Charts\n\nThe 'wykresy' function plots the histogram, probability density distribution, and cumulative distribution of each variable. Normal distributions are assumed for simplicity.\n\n```{python}\n\ndef wykresy(i, x, sd):\n        plt.subplot(1, 3, 1)\n        plt.hist(np.random.normal(x[i], sd[i], 10000), bins=30, alpha=0.7, color=\"#54698a\")\n        plt.xlabel(f\"x{i+1}\")\n        plt.ylabel(\"frequency\")\n        plt.title(\"Histogram\")\n\n        xx = np.linspace(x[i] - 4 * sd[i], x[i] + 4 * sd[i], 100)\n        yy = norm.pdf(xx, x[i], sd[i])\n        plt.subplot(1, 3, 2)\n        plt.plot(xx, yy, lw=2, color=\"#54698a\")\n        plt.xlabel(f\"x{i+1}\")\n        plt.ylabel(\"probability density\")\n        plt.title(f\"Distribution of x{i+1}\")\n\n        yy = norm.cdf(xx, x[i], sd[i])\n        plt.subplot(1, 3, 3)\n        plt.plot(xx, yy, lw=2, color=\"#54698a\")\n        plt.xlabel(f\"x{i+1}\")\n        plt.ylabel(\"cumulative probability\")\n        plt.title(\"cdf\")\n        plt.show()\n\n```\n\n## Monte Carlo\n\nFunction that implements a Monte Carlo experiment. Formal parameters: n - number of variables $x_i$, nn - number of draws, x - mean values of the $x_i$, sd - their standard deviations, parametry - parameters of the function describing the model. The function uses a random number generator and the inverse function of the cumulative distribution function.\n\n```{python}\n\ndef monte_carlo(n, nn, x, sd, parametry):\n    np.random.seed(7)\n    y = np.zeros(nn)\n    for i in range(nn):\n        los_x = np.zeros(n)\n        for j in range(n):\n            los_x[j] = norm.ppf(np.random.rand(), x[j], sd[j])\n        y[i] = funkcja_model(parametry, los_x)\n    return y\n\n```\n\n# The data\n\nExample data.\n\n```{python}\nimport pandas as pd\nfrom itables import show    \n\nn = 3  # the number of independent variables\nparametry = [1, 2, 3, 4]\n\nx1 = 10\nsd1 = 0.51\n\nx2 = 15\nsd2 = 0.5\n\nx3 = 5\nsd3 = 0.1\n\nx = [x1, x2, x3]\nsd = [sd1, sd2, sd3]\n\ndane_x = pd.DataFrame(x)\ndane_sd = pd.DataFrame(sd)\n\ndane = pd.concat([dane_x, dane_sd], axis=1)\ndane.columns = ['x','std']\ndane.index = ['x1','x2','x3']\n\nshow(np.transpose(dane))\n\n```\n\n## Data distributions\n\nInput data distributions.\n\n```{python}\n#| label: fig-1\n#| fig-cap: \n#|   - \"Distribution of x1\"\n#|   - \"Distribution of x2\"\n#|   - \"Distribution of x3\"\n#| fig-cap-location: margin\n#| message: false\n#| warning: false\n\nfor i in range(n):\n    wykresy(i, x, sd)\n\n```\n\n# Monte Carlo Simulation\n\nNumber of draws assumed: $nn = 100 \\: 000$.\n\n## Monte Carlo simulation results\n\n```{python}\n\nnn = 100000  # the number od draws\n\ny = monte_carlo(n, nn, x, sd, parametry)  #  Monte Carlo simulation\n\n```\n\nAll the results of the model calculations are shown in @fig-2 . Their mean and standard deviation are given below.\n\n```{python}\n#| label: fig-2\n#| fig-cap: \"Results of sampling in Monte Carlo simulation \"\n#| fig-cap-location: margin\n#| message: false\n#| warning: false\n\nplt.scatter(range(len(y)), y, s=0.5, color=\"#54698a\")\nplt.xlabel(\"draw number\")\nplt.ylabel(\"y\")\nplt.title(\"Summary Result of Monte Carlo Simulation\")\nplt.show()\n\ny_av = np.mean(y)\nsd_y = np.std(y)\n\nprint(\"The resulting mean y_av = \", round(y_av,2))\nprint(\"Standard deviation sd_y = \", round(sd_y,2))\n\n```\n\n## Distribution of the resulting quantity\n\n@fig-3 shows a histogram and a $y$ quantile plot for visual evaluation of the distribution.\n\n### Histogram and quantile plot\n\nThe histogram allows you to assess the symmetry of the distribution, and the quantile plot allows you to assess whether the distribution is normal.\n\n```{python}\n#| label: fig-3\n#| fig-cap: \"Visual check of the normality of the y-distribution \"\n#| fig-cap-location: margin\n#| message: false\n#| warning: false\n\nplt.subplot(1, 2, 1)\nplt.hist(y, bins=30, color=\"#54698a\", alpha=0.7)\nplt.title(\"Histogram of y\")\nplt.ylabel(\"frequency\")\nplt.xlabel(\"y\")\n\nyy = (y - y_av) / sd_y  # Standardization of distribution\n\nplt.subplot(1, 2, 2)\nplt.scatter(np.sort(norm.ppf(np.linspace(0.01, 0.99, len(yy)))), np.sort(yy), s=0.5, color=\"#54698a\")\nplt.title('Quantile plot Q-Q')\nplt.xlabel('quantiles of the normal distribution')\nplt.ylabel('quantiles of the y-distribution')\nplt.plot([min(yy), max(yy)], [min(yy), max(yy)], color='#96cdf2')\nplt.show()\n\n```\n\n### Check the normality of the y-distribution\n\nThe $y$ has 100,000 elements, so the Kolmogorov-Smirnov test was used to assess the normality of its distribution..\n\n```{python}\n\n# Kolmogorow-Smirnow test\nksx = ks_2samp(y, np.random.normal(y_av, sd_y, len(y)))\nksxp = ksx.pvalue\nprint(\" p_value = \", round(ksx.pvalue,4))\n\nif ksxp < 0.05:\n    print(\"\\nAccording to the Kolmogorov-Smirnov test, the distribution of y-size is not normal,\\nbecause the probability of falsely negating its normality  \\np_value is < 0.05. \")\nelse:\n    print(\"According to the Kolmogorov-Smirnov test, the distribution of y is normal (the normality of the distribution cannot be denied)\\n\\n\")\n\n# Shapiro-Wilka test\nif nn <= 5000:\n    swx = shapiro(y)\n    print(swx)\n\n    swxp = swx.pvalue\n\n    if swxp < 0.05:\n        print(\"According to the Shapiro-Wilk test, the distribution of y-size is not normal\\n\\n\")\n    else:\n        print(\"According to the Shapiro-Wilk test, the distribution of y is normal (the normality of the distribution cannot be denied)\\n\\n\")\n\n```\n\n### The distribution of y\n\n@fig-4 represents the obtained probability density distribution $y$ and its cumulative distribution function.\n\n```{python}\n#| label: fig-4\n#| fig-cap: \"The distribution of y \"\n#| tbl-cap-location: margin\n#| message: false\n#| warning: false\n\nplt.subplot(1, 2, 1)\nd = np.histogram(y, bins=50, density=True)\nplt.title(\"The distribution of y\")\nplt.xlabel(\"y\")\nplt.ylabel(\"probability density\")\nplt.plot(d[1][:-1], d[0], color=\"#54698a\", lw=2)\n\nplt.subplot(1, 2, 2)\nplt.plot(d[1][:-1], np.cumsum(d[0]) / max(np.cumsum(d[0])), lw=2, color=\"#54698a\")\nplt.xlabel(\"y\")\nplt.ylabel(\"cumulative probability\")\nplt.title(\"Cumulative distribution function\")\nplt.show()\n\n```\n\n### Median, skewness and kurtosis\n\nThe median, skewness, and kurtosis of $y$ were determined to further assess the deviation from the normal distribution.\n\n```{python}\n\nfrom scipy.stats import skew\nfrom scipy.stats import kurtosis\n\n\nmed = np.median(y,  axis=0)\nprint(\"Median = \", round(med,2))\n\nsk = skew(y, axis=0, bias=True)\nprint(\"Skewness = \", round(sk,5))\n\nkurt = kurtosis(y, fisher=True, axis=0, bias=True) # kurtoza = 0 dla rozkłądu Gaussa\nprint(\"Kortosis = \", round(kurt,5))\n\nq_025 = np.quantile(y, 0.025)\nq_975 = np.quantile(y, 0.975)\n\nprint(\"0.025 quantile = \", round(q_025,2))\nprint(\"0.975 quantile = \", round(q_975,2))\n```\n\n# Conclusion\n\nAll inputs $x_1,\\: x_2$ and $x_3$ follow a Gaussian distribution. The distribution of the result $y$ deviates slightly from the normal distribution. The skewness is small. The median is close to the mean. The kurtosis is slightly positive, which means that the distribution is slightly narrower than normal. The deviation from the normal distribution occurs in the 'tails' of the distribution, as can be clearly seen in the Q-Q plot in @fig-3 . The results are slightly more assembled around the mean than with the Gaussian distribution.\n\nThe variability of the data $x_1,\\: x_2$ and $x_3$ means that the result of the model's performance will be in the range of 596.13 to 771.98 with 95% confidence, i.e. $y = 682.4_{-86.3}^{+89.6}$.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"index_en.html"},"language":{"toc-title-document":"Spis treści","toc-title-website":"Na tej stronie","related-formats-title":"Inne formaty","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Źródło","other-links-title":"Inne odnośniki","code-links-title":"Linki do kodu","launch-dev-container-title":"Uruchom Dev Container","launch-binder-title":"Uruchom Binder","article-notebook-label":"Notatnik artykułu","notebook-preview-download":"Pobierz notatnik","notebook-preview-download-src":"Pobierz kod źródłowy","notebook-preview-back":"Powrót do artykułu","manuscript-meca-bundle":"Archiwum MECA","section-title-abstract":"Abstrakt","section-title-appendices":"Załączniki","section-title-footnotes":"Przypisy","section-title-references":"Bibliografia","section-title-reuse":"Licencja","section-title-copyright":"Prawa autorskie","section-title-citation":"Cytowanie","appendix-attribution-cite-as":"W celu atrybucji, proszę cytować tę pracę jako:","appendix-attribution-bibtex":"cytowanie BibTeX:","title-block-author-single":"Autor","title-block-author-plural":"Autorzy","title-block-affiliation-single":"Afiliacja","title-block-affiliation-plural":"Afiliacje","title-block-published":"Opublikowano","title-block-modified":"Zmodyfikowano","title-block-keywords":"Słowa kluczowe","callout-tip-title":"Wskazówka","callout-note-title":"Adnotacja","callout-warning-title":"Ostrzeżenie","callout-important-title":"Ważne","callout-caution-title":"Zagrożenie","code-summary":"Kod","code-tools-menu-caption":"Kod","code-tools-show-all-code":"Pokaż cały kod","code-tools-hide-all-code":"Ukryj cały kod","code-tools-view-source":"Pokaż źródło","code-tools-source-code":"Kod źródłowy","tools-share":"Share","tools-download":"Download","code-line":"Linia","code-lines":"Linie","copy-button-tooltip":"Kopiuj do schowka","copy-button-tooltip-success":"Skopiowano!","repo-action-links-edit":"Edytuj tę stronę","repo-action-links-source":"Pokaż źródło","repo-action-links-issue":"Zgłoś problem","back-to-top":"Powrót do góry","search-no-results-text":"Brak wyników","search-matching-documents-text":"dopasowane dokumenty","search-copy-link-title":"Kopiuj link do wyszukiwania","search-hide-matches-text":"Ukryj dodatkowe dopasowania","search-more-match-text":"więcej dopasowań w tym dokumencie","search-more-matches-text":"więcej dopasowań w tym dokumencie","search-clear-button-title":"Wyczyść","search-text-placeholder":"","search-detached-cancel-button-title":"Anuluj","search-submit-button-title":"Zatwierdź","search-label":"Szukaj","toggle-section":"Przełącz sekcję","toggle-sidebar":"Przełącz pasek boczny","toggle-dark-mode":"Przełącz tryb ciemny","toggle-reader-mode":"Przełącz tryb czytnika","toggle-navigation":"Przełącz nawigację","crossref-fig-title":"Rysunek","crossref-tbl-title":"Tabela","crossref-lst-title":"Wykaz","crossref-thm-title":"Twierdzenie","crossref-lem-title":"Lemat","crossref-cor-title":"Wniosek","crossref-prp-title":"Proposition","crossref-cnj-title":"Przypuszczenie","crossref-def-title":"Definicja","crossref-exm-title":"Przykład","crossref-exr-title":"Ćwiczenie","crossref-ch-prefix":"Rozdział","crossref-apx-prefix":"Załącznik","crossref-sec-prefix":"Sekcja","crossref-eq-prefix":"Równanie","crossref-lof-title":"Spis rycin","crossref-lot-title":"Spis tabel","crossref-lol-title":"Spis wykazów","environment-proof-title":"Dowód","environment-remark-title":"Komentarz","environment-solution-title":"Rozwiązanie","listing-page-order-by":"Sortuj według","listing-page-order-by-default":"Domyślnie","listing-page-order-by-date-asc":"Od najstarszych","listing-page-order-by-date-desc":"Od najnowszych","listing-page-order-by-number-desc":"Od największych","listing-page-order-by-number-asc":"Od najmniejszych","listing-page-field-date":"Data","listing-page-field-title":"Tytuł","listing-page-field-description":"Opis","listing-page-field-author":"Autor","listing-page-field-filename":"Nazwa pliku","listing-page-field-filemodified":"Zmodyfikowano","listing-page-field-subtitle":"Podtytuł","listing-page-field-readingtime":"Czas czytania","listing-page-field-wordcount":"Licznik Słów","listing-page-field-categories":"Kategorie","listing-page-minutes-compact":"{0} min","listing-page-category-all":"Wszystkie","listing-page-no-matches":"Brak pasujących","listing-page-words":"{0} słów"},"metadata":{"lang":"pl","fig-responsive":true,"quarto-version":"1.4.553","roject":{"type":"website"},"theme":"cosmo","title-block-banner":"#54698a","title-block-banner-color":"#dee1e3","title":"Monte Carlo simulation","subtitle":"Example","author":"Michał Kołodziejczyk","date":"now","editor":"visual","jupyter":"python3","cap-location":"margin","toc-location":"left","tbl-pos":"H","code-summary":"Script:"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}